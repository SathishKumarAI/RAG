{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Exploration Notebook\n",
        "\n",
        "This notebook explores raw data, demonstrates preprocessing and chunking functions, and shows examples of chunks with metadata.\n",
        "\n",
        "## Goals\n",
        "- Explore raw document data\n",
        "- Test preprocessing and chunking functions\n",
        "- Visualize chunks and metadata\n",
        "- Understand data characteristics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Imports successful!\n"
          ]
        }
      ],
      "source": [
        "# Import required modules\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add src to path\n",
        "sys.path.insert(0, str(Path(\"../src\").resolve()))\n",
        "\n",
        "from utils.io_utils import read_text, list_files\n",
        "from utils.rag_preprocessing import clean_text, chunk_text, enrich_metadata\n",
        "from utils.validation import validate_document\n",
        "\n",
        "print(\"✅ Imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Explore Raw Data\n",
        "\n",
        "Let's first see what documents we have available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1 files:\n",
            "  - Text Chunking.pdf\n"
          ]
        }
      ],
      "source": [
        "# List available documents with extended file types\n",
        "data_dir = Path(\"../data/raw\")\n",
        "file_types = [\"*.txt\", \"*.pdf\", \"*.docx\", \"*.json\", \"*.csv\", \"*.md\"]\n",
        "\n",
        "if data_dir.exists():\n",
        "    all_files = []\n",
        "    for p in file_types:\n",
        "        all_files += list_files(data_dir, pattern=p)\n",
        "\n",
        "    print(f\"Found {len(all_files)} files:\")\n",
        "    for f in all_files[:10]:\n",
        "        print(f\"  - {f.name}\")\n",
        "\n",
        "else:\n",
        "    print(\"Data directory not found. Creating sample data...\")\n",
        "    data_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    samples = {\n",
        "        \"sample.txt\": \"This is sample text about RAG and retrieval.\",\n",
        "        \"sample.json\": '{\"topic\":\"RAG\",\"desc\":\"Retrieval-Augmented Generation\"}',\n",
        "        \"sample.md\": \"# RAG Example\\nRAG retrieves context before generation.\"\n",
        "    }\n",
        "\n",
        "    for name, content in samples.items():\n",
        "        with open(data_dir / name, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(content)\n",
        "\n",
        "    print(\"Created sample files:\", \", \".join(samples.keys()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load and Clean Text\n",
        "\n",
        "Let's load a document and clean it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded: Text Chunking.pdf\n",
            "Raw text:\n",
            "%PDF-1.5\n",
            "%\n",
            "87 0 obj\n",
            "<< /Filter /FlateDecode /Length 4363 >>\n",
            "stream\n",
            "xÚ:Ûã¸ïý\u0015~\n",
            "h+¢¨ë¾u\u0006Ù\u000ef0Év-fJ=°$ÚfJ<ÔÝ5ûó{\u000eÏ!E»\u001c`BwòÜoÊvÇ]¶ûéÃï\u001f\u0004üf;±Û·Ù®.Ë´Ìê]wþðôízøÛ.KeÛì¾¹eç]%rø\u001dv_>üãCÆüåñÃÌ]¶U\n",
            "\n",
            "============================================================\n",
            "\n",
            "Cleaned text:\n",
            "%PDF-1.5 % 87 0 obj << /Filter /FlateDecode /Length 4363 >> stream xÚ:Ûã¸ïý\u0015~ h+¢¨ë¾u\u0006Ù\u000ef0Év-fJ=°$ÚfJ<ÔÝ5ûó{\u000eÏ!E» `BwòÜoÊvÇ]¶ûéÃï \u0004üf;±Û·Ù®.Ë´Ìê]wþðôízøÛ.KeÛì¾¹eç]%rø v_>üãCÆüåñÃÌ]¶U\n"
          ]
        }
      ],
      "source": [
        "# Load and clean text from any supported file\n",
        "supported = [\"*.txt\", \"*.pdf\", \"*.docx\", \"*.json\", \"*.md\", \"*.csv\"]\n",
        "files = []\n",
        "for p in supported:\n",
        "    files += list_files(data_dir, pattern=p)\n",
        "\n",
        "if files:\n",
        "    file = files[0]\n",
        "    raw_text = read_text(file)\n",
        "    print(f\"Loaded: {file.name}\\nRaw text:\\n{raw_text[:200]}\")\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "    cleaned = clean_text(\n",
        "        raw_text,\n",
        "        remove_html=True,\n",
        "        normalize_whitespace=True,\n",
        "    )\n",
        "    print(\"Cleaned text:\\n\" + cleaned[:200])\n",
        "else:\n",
        "    print(\"No supported files found in data directory.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Chunk Text\n",
        "\n",
        "Now let's chunk the cleaned text using different strategies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fixed-size chunks: 11\n",
            "Recursive chunks: 15\n",
            "Sentence chunks: 15\n",
            "\n",
            "First chunk (fixed-size):\n",
            "\n",
            "Retrieval-Augmented Generation (RAG) is a powerful technique that combines the strengths of information retrieval and language generation.\n",
            "\n",
            "RAG works...\n"
          ]
        }
      ],
      "source": [
        "# Create a longer sample text for chunking\n",
        "long_text = \"\"\"\n",
        "Retrieval-Augmented Generation (RAG) is a powerful technique that combines the strengths of information retrieval and language generation.\n",
        "\n",
        "RAG works by first retrieving relevant documents from a knowledge base, then using those documents as context for generating answers.\n",
        "\n",
        "The key advantage of RAG is that it can provide accurate, up-to-date information by grounding generation in retrieved documents.\n",
        "\n",
        "RAG systems typically consist of three main components: a retriever, a generator, and a knowledge base.\n",
        "\n",
        "The retriever finds relevant documents, the generator produces answers based on those documents, and the knowledge base stores the information.\n",
        "\"\"\" * 3  # Repeat to make it longer\n",
        "\n",
        "# Chunk using different strategies\n",
        "chunks_fixed = chunk_text(long_text, strategy=\"fixed_size\", chunk_size=200, chunk_overlap=20)\n",
        "chunks_recursive = chunk_text(long_text, strategy=\"recursive\", chunk_size=200, chunk_overlap=20)\n",
        "chunks_sentence = chunk_text(long_text, strategy=\"sentence\", chunk_size=200, chunk_overlap=20)\n",
        "\n",
        "print(f\"Fixed-size chunks: {len(chunks_fixed)}\")\n",
        "print(f\"Recursive chunks: {len(chunks_recursive)}\")\n",
        "print(f\"Sentence chunks: {len(chunks_sentence)}\")\n",
        "\n",
        "print(\"\\nFirst chunk (fixed-size):\")\n",
        "print(chunks_fixed[0][:150] + \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Enrich Chunks with Metadata\n",
        "\n",
        "Let's create chunk dictionaries with metadata.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First chunk with metadata:\n",
            "Text: \n",
            "Retrieval-Augmented Generation (RAG) is a powerful technique that combines the strengths of informa...\n",
            "\n",
            "Metadata:\n",
            "  source: sample.txt\n",
            "  page: 1\n",
            "  section: Introduction\n",
            "  timestamp: 2025-11-29T03:02:50.406629\n",
            "  chunk_index: 0\n",
            "\n",
            "✅ Chunk validation passed!\n"
          ]
        }
      ],
      "source": [
        "# Create chunks with metadata\n",
        "chunks = []\n",
        "for i, chunk_text_content in enumerate(chunks_fixed):\n",
        "    chunk = {\n",
        "        \"text\": chunk_text_content,\n",
        "        \"metadata\": {}\n",
        "    }\n",
        "    chunk = enrich_metadata(\n",
        "        chunk,\n",
        "        source=\"sample.txt\",\n",
        "        page=1,\n",
        "        section=\"Introduction\",\n",
        "        chunk_index=i\n",
        "    )\n",
        "    chunks.append(chunk)\n",
        "\n",
        "# Display first chunk with metadata\n",
        "print(\"First chunk with metadata:\")\n",
        "print(f\"Text: {chunks[0]['text'][:100]}...\")\n",
        "print(f\"\\nMetadata:\")\n",
        "for key, value in chunks[0]['metadata'].items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# Validate chunk\n",
        "try:\n",
        "    validate_document(chunks[0])\n",
        "    print(\"\\n✅ Chunk validation passed!\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ Chunk validation failed: {e}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "RAG",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
