{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Basic Multimodal RAG Pipeline Tutorial\n",
        "\n",
        "This notebook implements a simple, beginner-friendly multimodal RAG pipeline that:\n",
        "- Parses PDFs into atomic elements (text, titles, tables, images)\n",
        "- Chunks content by title\n",
        "- Handles hybrid chunks (text + tables/images)\n",
        "- Builds a vector store for retrieval\n",
        "- Demonstrates basic RAG retrieval\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1: Environment Setup & Installation\n",
        "\n",
        "First, install the required packages. Run this in your terminal:\n",
        "\n",
        "```bash\n",
        "pip install unstructured[pdf]\n",
        "pip install langchain-core langchain-chroma langchain-openai\n",
        "pip install python-dotenv\n",
        "```\n",
        "\n",
        "**System Dependencies (install separately):**\n",
        "- `poppler` (for PDF processing)\n",
        "- `tesseract` (for OCR)\n",
        "- `libmagic` (for file type detection)\n",
        "\n",
        "On Ubuntu/Debian:\n",
        "```bash\n",
        "sudo apt-get install poppler-utils tesseract-ocr libmagic1\n",
        "```\n",
        "\n",
        "On macOS:\n",
        "```bash\n",
        "brew install poppler tesseract libmagic\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here‚Äôs a unified **System Dependencies** section you can drop straight into your README, with install instructions for **Windows**, **Ubuntu/WSL**, and **macOS**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß© System Dependencies (All OS)\n",
        "\n",
        "This project needs three native tools:\n",
        "\n",
        "* **poppler** ‚Äì PDF processing (e.g., `pdftotext`, `pdfimages`)\n",
        "* **tesseract** ‚Äì OCR for scanned PDFs\n",
        "* **libmagic** ‚Äì file type detection (`python-magic` uses this)\n",
        "\n",
        "Below are OS-specific install steps.\n",
        "\n",
        "---\n",
        "\n",
        "### ü™ü Windows\n",
        "\n",
        "#### 1. Poppler\n",
        "\n",
        "1. Download latest Poppler build (ZIP) from:\n",
        "   [https://github.com/oschwartz10612/poppler-windows/releases](https://github.com/oschwartz10612/poppler-windows/releases)\n",
        "2. Extract to a permanent folder, e.g.:\n",
        "   `C:\\Tools\\poppler`\n",
        "3. Add this to your **System PATH**:\n",
        "   `C:\\Tools\\poppler\\Library\\bin`\n",
        "\n",
        "Verify in **PowerShell**:\n",
        "\n",
        "```powershell\n",
        "pdftotext -v\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. Tesseract OCR\n",
        "\n",
        "1. Download Windows installer (UB Mannheim build recommended):\n",
        "   [https://github.com/UB-Mannheim/tesseract/wiki](https://github.com/UB-Mannheim/tesseract/wiki)\n",
        "2. Install (default path):\n",
        "   `C:\\Program Files\\Tesseract-OCR`\n",
        "3. Add to **PATH**:\n",
        "   `C:\\Program Files\\Tesseract-OCR`\n",
        "\n",
        "Verify:\n",
        "\n",
        "```powershell\n",
        "tesseract --version\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. libmagic\n",
        "\n",
        "Windows uses a bundled Python version:\n",
        "\n",
        "```bash\n",
        "pip install python-magic-bin\n",
        "```\n",
        "\n",
        "Quick test:\n",
        "\n",
        "```bash\n",
        "python -c \"import magic; print(magic.from_buffer(b'hello'))\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üêß Ubuntu / Debian / WSL2 (Ubuntu)\n",
        "\n",
        "From your **WSL/Ubuntu terminal**:\n",
        "\n",
        "```bash\n",
        "sudo apt-get update\n",
        "sudo apt-get install -y \\\n",
        "  poppler-utils \\\n",
        "  tesseract-ocr \\\n",
        "  libmagic1\n",
        "```\n",
        "\n",
        "> Optional language packs for Tesseract (e.g., English):\n",
        ">\n",
        "> ```bash\n",
        "> sudo apt-get install -y tesseract-ocr-eng\n",
        "> ```\n",
        "\n",
        "Verify:\n",
        "\n",
        "```bash\n",
        "pdftotext -v\n",
        "tesseract --version\n",
        "python - << 'EOF'\n",
        "import magic\n",
        "print(magic.from_buffer(b'hello'))\n",
        "EOF\n",
        "```\n",
        "\n",
        "*(If `magic` is missing, run: `pip install python-magic` in your env.)*\n",
        "\n",
        "---\n",
        "\n",
        "### üçè macOS (with Homebrew)\n",
        "\n",
        "Make sure you have **Homebrew** installed first: [https://brew.sh](https://brew.sh)\n",
        "\n",
        "Then:\n",
        "\n",
        "```bash\n",
        "brew update\n",
        "brew install poppler tesseract libmagic\n",
        "```\n",
        "\n",
        "Verify:\n",
        "\n",
        "```bash\n",
        "pdftotext -v\n",
        "tesseract --version\n",
        "python - << 'EOF'\n",
        "import magic\n",
        "print(magic.from_buffer(b'hello'))\n",
        "EOF\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üîé Final Sanity Check (All Platforms)\n",
        "\n",
        "In your activated Python/conda env:\n",
        "\n",
        "```bash\n",
        "pip install \"unstructured[pdf]\" langchain-core langchain-chroma langchain-openai python-dotenv\n",
        "python - << 'EOF'\n",
        "import unstructured, langchain_core, dotenv\n",
        "print(\"Python deps OK\")\n",
        "EOF\n",
        "```\n",
        "\n",
        "If all verification commands succeed, your **PDF/OCR/file-type stack** is ready on Windows, WSL, and macOS.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\SuryaDeva\\anaconda3\\envs\\RAG\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All imports successful!\n"
          ]
        }
      ],
      "source": [
        "# Import all required libraries\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Unstructured library for PDF parsing\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "from unstructured.chunking.title import chunk_by_title\n",
        "\n",
        "# LangChain components\n",
        "from langchain_core.documents import Document\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "# Load environment variables (for API keys)\n",
        "load_dotenv()\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2: Partition PDF into Atomic Elements\n",
        "\n",
        "This function extracts all elements from a PDF: text, titles, tables, and images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def partition_document(file_path: str) -> List[Any]:\n",
        "    \"\"\"\n",
        "    Partition a PDF into atomic elements (text, titles, tables, images).\n",
        "    \n",
        "    Args:\n",
        "        file_path: Path to the PDF file\n",
        "        \n",
        "    Returns:\n",
        "        List of unstructured elements\n",
        "    \"\"\"\n",
        "    print(f\"üìÑ Partitioning PDF: {file_path}\")\n",
        "    \n",
        "    # Partition PDF with high-resolution strategy\n",
        "    # This extracts text, tables, and images\n",
        "    elements = partition_pdf(\n",
        "        filename=file_path,\n",
        "        strategy=\"hi_res\",  # High resolution for better accuracy\n",
        "        infer_table_structure=True,  # Extract tables as structured data\n",
        "        extract_image_block_types=[\"Image\"],  # Extract images\n",
        "        extract_image_block_to_payload=True,  # Include image data\n",
        "    )\n",
        "    \n",
        "    print(f\"‚úÖ Found {len(elements)} elements in the PDF\")\n",
        "    return elements\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def inspect_element_types(elements: List[Any]) -> None:\n",
        "    \"\"\"\n",
        "    Helper function to inspect what types of elements were found.\n",
        "    \"\"\"\n",
        "    # Get unique element types\n",
        "    element_types = set()\n",
        "    for elem in elements:\n",
        "        element_types.add(elem.__class__.__name__)\n",
        "    \n",
        "    print(f\"\\nüìä Unique element types found: {sorted(element_types)}\")\n",
        "    \n",
        "    # Find examples of each type\n",
        "    examples = {}\n",
        "    for elem in elements:\n",
        "        elem_type = elem.__class__.__name__\n",
        "        if elem_type not in examples:\n",
        "            examples[elem_type] = elem\n",
        "    \n",
        "    # Print examples\n",
        "    print(\"\\nüìù Example elements:\")\n",
        "    for elem_type, elem in examples.items():\n",
        "        print(f\"\\n--- {elem_type} ---\")\n",
        "        elem_dict = elem.to_dict()\n",
        "        # Show a preview (first 200 chars)\n",
        "        if 'text' in elem_dict:\n",
        "            text_preview = elem_dict['text'][:200]\n",
        "            print(f\"Text preview: {text_preview}...\")\n",
        "        print(f\"Full dict keys: {list(elem_dict.keys())}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 3: Chunk Elements by Title\n",
        "\n",
        "This function groups elements into chunks based on titles, following the pattern:\n",
        "\"title + related paragraphs + any tables/images\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chunk_elements_by_title(elements: List[Any]) -> List[Any]:\n",
        "    \"\"\"\n",
        "    Chunk elements by title, grouping related content together.\n",
        "    \n",
        "    Args:\n",
        "        elements: List of unstructured elements\n",
        "        \n",
        "    Returns:\n",
        "        List of chunks\n",
        "    \"\"\"\n",
        "    print(f\"\\nüî™ Chunking {len(elements)} elements by title...\")\n",
        "    \n",
        "    # Chunk by title with reasonable size limits\n",
        "    chunks = chunk_by_title(\n",
        "        elements,\n",
        "        max_characters=3000,  # Maximum characters per chunk\n",
        "        new_after_n_chars=2400,  # Start new chunk after this many chars\n",
        "        combine_text_under_n_chars=500,  # Combine small chunks under this size\n",
        "    )\n",
        "    \n",
        "    print(f\"‚úÖ Created {len(chunks)} chunks\")\n",
        "    return chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def inspect_chunk(chunk: Any, chunk_idx: int = 0) -> None:\n",
        "    \"\"\"\n",
        "    Helper function to inspect a single chunk.\n",
        "    \"\"\"\n",
        "    print(f\"\\nüì¶ Inspecting chunk {chunk_idx}:\")\n",
        "    print(f\"Text preview (first 300 chars): {chunk.text[:300]}...\")\n",
        "    \n",
        "    # Check what types of elements are in this chunk\n",
        "    if hasattr(chunk, 'metadata') and hasattr(chunk.metadata, 'orig_elements'):\n",
        "        element_types = [elem.__class__.__name__ for elem in chunk.metadata.orig_elements]\n",
        "        print(f\"Element types in chunk: {element_types}\")\n",
        "    else:\n",
        "        print(\"Note: orig_elements metadata not available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 4: Separate Content Types\n",
        "\n",
        "This function separates text, tables, and images from a chunk.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def separate_content_types(chunk: Any) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Separate a chunk into text, tables, and images.\n",
        "    \n",
        "    Args:\n",
        "        chunk: A chunk from chunk_by_title\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary with 'text', 'tables', 'images', and 'types'\n",
        "    \"\"\"\n",
        "    result = {\n",
        "        \"text\": chunk.text,\n",
        "        \"tables\": [],\n",
        "        \"images\": [],\n",
        "        \"types\": [\"text\"]  # Always has text\n",
        "    }\n",
        "    \n",
        "    # Check if chunk has original elements metadata\n",
        "    if hasattr(chunk, 'metadata') and hasattr(chunk.metadata, 'orig_elements'):\n",
        "        for elem in chunk.metadata.orig_elements:\n",
        "            elem_type = elem.__class__.__name__\n",
        "            \n",
        "            # Extract tables\n",
        "            if elem_type == \"Table\":\n",
        "                if hasattr(elem, 'metadata') and hasattr(elem.metadata, 'text_as_html'):\n",
        "                    table_html = elem.metadata.text_as_html\n",
        "                    result[\"tables\"].append(table_html)\n",
        "                    if \"table\" not in result[\"types\"]:\n",
        "                        result[\"types\"].append(\"table\")\n",
        "            \n",
        "            # Extract images\n",
        "            elif elem_type == \"Image\":\n",
        "                if hasattr(elem, 'metadata') and hasattr(elem.metadata, 'image_base64'):\n",
        "                    image_b64 = elem.metadata.image_base64\n",
        "                    result[\"images\"].append(image_b64)\n",
        "                    if \"image\" not in result[\"types\"]:\n",
        "                        result[\"types\"].append(\"image\")\n",
        "    \n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 5: Simple Summarization for Hybrid Chunks\n",
        "\n",
        "This is a basic rule-based summarizer (no LLM required).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def summarize_chunk_basically(text: str, tables: list, images: list) -> str:\n",
        "    \"\"\"\n",
        "    Create a simple text summary for hybrid chunks (text + tables/images).\n",
        "    \n",
        "    This is a basic rule-based summarizer - no LLM call needed.\n",
        "    \n",
        "    Args:\n",
        "        text: The text content of the chunk\n",
        "        tables: List of HTML table strings\n",
        "        images: List of base64 image strings\n",
        "        \n",
        "    Returns:\n",
        "        Summary string suitable for embedding\n",
        "    \"\"\"\n",
        "    # Take first 500 characters of text as summary\n",
        "    summary = text[:500]\n",
        "    \n",
        "    # Add notes about tables and images\n",
        "    notes = []\n",
        "    if tables:\n",
        "        notes.append(f\"{len(tables)} table(s)\")\n",
        "    if images:\n",
        "        notes.append(f\"{len(images)} image(s)\")\n",
        "    \n",
        "    if notes:\n",
        "        summary += f\" [contains {', '.join(notes)}]\"\n",
        "    \n",
        "    return summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 6: Build LangChain Documents\n",
        "\n",
        "Convert chunks into LangChain Document objects with proper metadata.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_documents_from_chunks(chunks: List[Any], file_path: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Convert chunks into LangChain Document objects.\n",
        "    \n",
        "    Args:\n",
        "        chunks: List of chunks from chunk_by_title\n",
        "        file_path: Path to the original PDF file\n",
        "        \n",
        "    Returns:\n",
        "        List of LangChain Document objects\n",
        "    \"\"\"\n",
        "    print(f\"\\nüìö Building LangChain Documents from {len(chunks)} chunks...\")\n",
        "    \n",
        "    documents = []\n",
        "    \n",
        "    for idx, chunk in enumerate(chunks):\n",
        "        # Separate content types\n",
        "        content_data = separate_content_types(chunk)\n",
        "        \n",
        "        # Determine page_content\n",
        "        if content_data[\"tables\"] or content_data[\"images\"]:\n",
        "            # Hybrid chunk: use summary\n",
        "            page_content = summarize_chunk_basically(\n",
        "                content_data[\"text\"],\n",
        "                content_data[\"tables\"],\n",
        "                content_data[\"images\"]\n",
        "            )\n",
        "        else:\n",
        "            # Pure text chunk: use raw text\n",
        "            page_content = content_data[\"text\"]\n",
        "        \n",
        "        # Build metadata\n",
        "        metadata = {\n",
        "            \"source\": file_path,\n",
        "            \"chunk_index\": idx,\n",
        "            \"types\": content_data[\"types\"],\n",
        "            \"raw_text\": content_data[\"text\"],\n",
        "            \"raw_tables_html\": content_data[\"tables\"],\n",
        "            \"raw_images_b64\": content_data[\"images\"],\n",
        "        }\n",
        "        \n",
        "        # Try to get page number if available\n",
        "        if hasattr(chunk, 'metadata') and hasattr(chunk.metadata, 'page_number'):\n",
        "            metadata[\"page_number\"] = chunk.metadata.page_number\n",
        "        elif hasattr(chunk, 'metadata') and hasattr(chunk.metadata, 'page'):\n",
        "            metadata[\"page_number\"] = chunk.metadata.page\n",
        "        \n",
        "        # Create LangChain Document\n",
        "        doc = Document(\n",
        "            page_content=page_content,\n",
        "            metadata=metadata\n",
        "        )\n",
        "        \n",
        "        documents.append(doc)\n",
        "    \n",
        "    print(f\"‚úÖ Created {len(documents)} LangChain Documents\")\n",
        "    \n",
        "    # Print some stats\n",
        "    pure_text = sum(1 for doc in documents if doc.metadata[\"types\"] == [\"text\"])\n",
        "    hybrid = len(documents) - pure_text\n",
        "    print(f\"   - Pure text chunks: {pure_text}\")\n",
        "    print(f\"   - Hybrid chunks (with tables/images): {hybrid}\")\n",
        "    \n",
        "    return documents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 7: Basic Vector Store + Retrieval Demo\n",
        "\n",
        "Build a vector store and demonstrate retrieval.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def build_vector_store(documents: List[Document], persist_directory: str = \"./chroma_db\") -> Chroma:\n",
        "#     \"\"\"\n",
        "#     Build a Chroma vector store from documents.\n",
        "    \n",
        "#     Args:\n",
        "#         documents: List of LangChain Documents\n",
        "#         persist_directory: Directory to persist the vector store\n",
        "        \n",
        "#     Returns:\n",
        "#         Chroma vector store\n",
        "#     \"\"\"\n",
        "#     print(f\"\\nüî® Building vector store from {len(documents)} documents...\")\n",
        "    \n",
        "#     # Initialize embeddings\n",
        "#     # Option 1: OpenAI embeddings (requires OPENAI_API_KEY in .env)\n",
        "#     # embeddings = OpenAIEmbeddings()\n",
        "    \n",
        "#     # Option 2: Use a small local model (uncomment if you prefer)\n",
        "#     # from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "#     # embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    \n",
        "#     # For this demo, we'll use OpenAI (make sure you have OPENAI_API_KEY set)\n",
        "#     embeddings = OpenAIEmbeddings()\n",
        "    \n",
        "#     # Create vector store\n",
        "#     vectorstore = Chroma.from_documents(\n",
        "#         documents=documents,\n",
        "#         embedding=embeddings,\n",
        "#         persist_directory=persist_directory\n",
        "#     )\n",
        "    \n",
        "#     print(f\"‚úÖ Vector store created and persisted to {persist_directory}\")\n",
        "#     return vectorstore\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def demo_retrieval(vectorstore: Chroma, question: str, k: int = 3) -> None:\n",
        "#     \"\"\"\n",
        "#     Demonstrate retrieval from the vector store.\n",
        "    \n",
        "#     Args:\n",
        "#         vectorstore: The Chroma vector store\n",
        "#         question: The query/question to search for\n",
        "#         k: Number of documents to retrieve\n",
        "#     \"\"\"\n",
        "#     print(f\"\\nüîç Query: {question}\")\n",
        "#     print(f\"Retrieving top {k} documents...\\n\")\n",
        "    \n",
        "#     # Perform similarity search\n",
        "#     results = vectorstore.similarity_search(question, k=k)\n",
        "    \n",
        "#     # Print results\n",
        "#     for idx, doc in enumerate(results, 1):\n",
        "#         print(f\"--- Result {idx} ---\")\n",
        "#         print(f\"Page number: {doc.metadata.get('page_number', 'N/A')}\")\n",
        "#         print(f\"Content types: {doc.metadata.get('types', [])}\")\n",
        "#         print(f\"Page content (first 300 chars): {doc.page_content[:300]}...\")\n",
        "#         print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 8: Orchestrator & Main Pipeline\n",
        "\n",
        "This is the main function that ties everything together.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main(pdf_path: str = None):\n",
        "    \"\"\"\n",
        "    Main orchestrator function that runs the entire pipeline.\n",
        "    \n",
        "    Args:\n",
        "        pdf_path: Path to the PDF file (default: looks for sample PDF)\n",
        "    \"\"\"\n",
        "    # Set default PDF path if not provided\n",
        "    if pdf_path is None:\n",
        "        # Try to find a PDF in the data/raw directory\n",
        "        default_paths = [\n",
        "            \"../data/raw/Text Chunking.pdf\",\n",
        "            \"./data/raw/Text Chunking.pdf\",\n",
        "            \"../rag-pipeline/data/raw/Text Chunking.pdf\",\n",
        "        ]\n",
        "        \n",
        "        pdf_path = None\n",
        "        for path in default_paths:\n",
        "            if os.path.exists(path):\n",
        "                pdf_path = path\n",
        "                break\n",
        "        \n",
        "        if pdf_path is None:\n",
        "            print(\"‚ùå No PDF found. Please provide a pdf_path argument.\")\n",
        "            print(\"Example: main(pdf_path='./docs/attention-is-all-you-need.pdf')\")\n",
        "            return\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"üöÄ Starting Multimodal RAG Pipeline\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Step 1: Partition PDF into elements\n",
        "    print(\"\\n[Step 1] Partitioning PDF...\")\n",
        "    elements = partition_document(pdf_path)\n",
        "    \n",
        "    # Inspect element types (optional, for debugging)\n",
        "    inspect_element_types(elements)\n",
        "    \n",
        "    # Step 2: Chunk elements by title\n",
        "    print(\"\\n[Step 2] Chunking by title...\")\n",
        "    chunks = chunk_elements_by_title(elements)\n",
        "    \n",
        "    # Inspect first chunk (optional, for debugging)\n",
        "    if chunks:\n",
        "        inspect_chunk(chunks[0], chunk_idx=0)\n",
        "    \n",
        "    # Step 3: Build LangChain Documents\n",
        "    print(\"\\n[Step 3] Building LangChain Documents...\")\n",
        "    documents = build_documents_from_chunks(chunks, pdf_path)\n",
        "    \n",
        "    # Step 4: Build vector store\n",
        "    print(\"\\n[Step 4] Building vector store...\")\n",
        "    # vectorstore = build_vector_store(documents)\n",
        "    \n",
        "    # Step 5: Demo retrieval\n",
        "    print(\"\\n[Step 5] Running retrieval demo...\")\n",
        "    \n",
        "    # Example questions (customize these for your PDF)\n",
        "    questions = [\n",
        "        \"What is the main topic of this document?\",\n",
        "        \"What are the key concepts discussed?\",\n",
        "    ]\n",
        "    \n",
        "    # for question in questions:\n",
        "    #     demo_retrieval(vectorstore, question, k=3)\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"‚úÖ Pipeline completed successfully!\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # return vectorstore, documents\n",
        "    return  documents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the Pipeline\n",
        "\n",
        "Execute the main function to run the entire pipeline. Make sure you have:\n",
        "1. A PDF file (update the path below)\n",
        "2. `OPENAI_API_KEY` set in your `.env` file (or environment variables)\n",
        "3. All required packages installed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "üöÄ Starting Multimodal RAG Pipeline\n",
            "============================================================\n",
            "\n",
            "[Step 1] Partitioning PDF...\n",
            "üìÑ Partitioning PDF: C:\\Users\\SuryaDeva\\Documents\\Certifications_202k\\Coding\\RAG_Mini\\rag-pipeline\\data\\raw\\Text Chunking.pdf\n",
            "Warning: No languages specified, defaulting to English.\n"
          ]
        }
      ],
      "source": [
        "# Run the pipeline!\n",
        "# Update the path to your PDF file\n",
        "# pdf_path = \"../data/raw/Text Chunking.pdf\"  # Change this to your PDF path\n",
        "pdf_path = rf\"C:\\Users\\SuryaDeva\\Documents\\Certifications_202k\\Coding\\RAG_Mini\\rag-pipeline\\data\\raw\\Text Chunking.pdf\"\n",
        "# Uncomment to run:\n",
        "# vectorstore, documents = main(pdf_path=pdf_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: Test Individual Components\n",
        "\n",
        "You can also test individual functions separately:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÑ Partitioning PDF: C:\\Users\\SuryaDeva\\Documents\\Certifications_202k\\Coding\\RAG_Mini\\rag-pipeline\\data\\raw\\Text Chunking.pdf\n",
            "Warning: No languages specified, defaulting to English.\n",
            "‚úÖ Found 196 elements in the PDF\n"
          ]
        }
      ],
      "source": [
        "# Example: Test partitioning only\n",
        "elements = partition_document(pdf_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Unique element types found: ['FigureCaption', 'Header', 'Image', 'ListItem', 'NarrativeText', 'Table', 'Text', 'Title']\n",
            "\n",
            "üìù Example elements:\n",
            "\n",
            "--- Text ---\n",
            "Text preview: 5...\n",
            "Full dict keys: ['type', 'element_id', 'text', 'metadata']\n",
            "\n",
            "--- Header ---\n",
            "Text preview: r a M 1 3 ] L C . s c [ 1 v 4 7 2 0 0 . 4 0 5 2...\n",
            "Full dict keys: ['type', 'element_id', 'text', 'metadata']\n",
            "\n",
            "--- NarrativeText ---\n",
            "Text preview: Text Chunking for Document Classification for Urban Systems Management using Large Language Models...\n",
            "Full dict keys: ['type', 'element_id', 'text', 'metadata']\n",
            "\n",
            "--- ListItem ---\n",
            "Text preview: * Corresponding author; email: steve.conrad@colostate.edu...\n",
            "Full dict keys: ['type', 'element_id', 'text', 'metadata']\n",
            "\n",
            "--- Title ---\n",
            "Text preview: Abstract...\n",
            "Full dict keys: ['type', 'element_id', 'text', 'metadata']\n",
            "\n",
            "--- Table ---\n",
            "Text preview: ments, taking as input StudySet, Codebook Algorithm 1 Whole Paper analysis of documents, 1: for Text ‚àà StudySet do taking as input StudySet, Codebook 2: Divide Text into TextChunks 1: for Text ‚àà Study...\n",
            "Full dict keys: ['type', 'element_id', 'text', 'metadata']\n",
            "\n",
            "--- Image ---\n",
            "Text preview: Chunk Whole Paper 100 90 80 2 ‚Äú| 70 = _ 60 g ¬© 50 ¬ß 100 8 90 Q < 80 z & oo 2. % ¬© 50 2 100 < 90 80 3 70 a 60 50 3 6 9 12 15, 3 6 9 12 15, Number of Executed Iterations...\n",
            "Full dict keys: ['type', 'element_id', 'text', 'metadata']\n",
            "\n",
            "--- FigureCaption ---\n",
            "Text preview: Figure 1. Internal Agreement of LLMs and Prompting Approach. The blue line shows the average internal agreement across all papers while the grey lines show the individual results of each paper....\n",
            "Full dict keys: ['type', 'element_id', 'text', 'metadata']\n"
          ]
        }
      ],
      "source": [
        "inspect_element_types(elements)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Step 2] Chunking by title...\n",
            "\n",
            "üî™ Chunking 196 elements by title...\n",
            "‚úÖ Created 24 chunks\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Chunk elements by title\n",
        "print(\"\\n[Step 2] Chunking by title...\")\n",
        "chunks = chunk_elements_by_title(elements)\n",
        "    \n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üì¶ Inspecting chunk 0:\n",
            "Text preview (first 300 chars): 5\n",
            "\n",
            "2025\n",
            "\n",
            "2\n",
            "\n",
            "0\n",
            "\n",
            "2\n",
            "\n",
            "r a M 1 3 ] L C . s c [ 1 v 4 7 2 0 0 . 4 0 5 2\n",
            "\n",
            ":\n",
            "\n",
            "v\n",
            "\n",
            "i\n",
            "\n",
            "X\n",
            "\n",
            "r\n",
            "\n",
            "a\n",
            "\n",
            "Text Chunking for Document Classification for Urban Systems Management using Large Language Models\n",
            "\n",
            "Joshua Rodriguez1‚Ä†, Om Sanan2‚Ä†, Guillermo Vizarreta-Luna1, Steven A. Conrad1*\n",
            "\n",
            "1 Department of Systems Engineering,...\n",
            "Element types in chunk: ['Text', 'Text', 'Text', 'Text', 'Text', 'Header', 'Text', 'Text', 'Text', 'Text', 'Text', 'Text', 'NarrativeText', 'NarrativeText', 'NarrativeText', 'ListItem', 'Text', 'Title', 'NarrativeText', 'Text']\n"
          ]
        }
      ],
      "source": [
        " # Inspect first chunk (optional, for debugging)\n",
        "if chunks:\n",
        "    inspect_chunk(chunks[0], chunk_idx=0)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Step 3] Building LangChain Documents...\n",
            "\n",
            "üìö Building LangChain Documents from 24 chunks...\n",
            "‚úÖ Created 24 LangChain Documents\n",
            "   - Pure text chunks: 17\n",
            "   - Hybrid chunks (with tables/images): 7\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Build LangChain Documents\n",
        "print(\"\\n[Step 3] Building LangChain Documents...\")\n",
        "documents = build_documents_from_chunks(chunks, pdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "This notebook implements a complete basic multimodal RAG pipeline:\n",
        "\n",
        "1. ‚úÖ **Environment setup** - Installation instructions\n",
        "2. ‚úÖ **PDF partitioning** - Extract atomic elements (text, tables, images)\n",
        "3. ‚úÖ **Title-based chunking** - Group related content\n",
        "4. ‚úÖ **Content type separation** - Identify text, tables, images\n",
        "5. ‚úÖ **Basic summarization** - Create summaries for hybrid chunks\n",
        "6. ‚úÖ **LangChain Documents** - Convert to standard format\n",
        "7. ‚úÖ **Vector store** - Build Chroma index with embeddings\n",
        "8. ‚úÖ **Retrieval demo** - Query and retrieve relevant chunks\n",
        "9. ‚úÖ **Orchestrator** - Main pipeline function\n",
        "\n",
        "**Next Steps:**\n",
        "- Add an LLM to generate final answers from retrieved chunks\n",
        "- Experiment with different chunking strategies\n",
        "- Try different embedding models\n",
        "- Add more sophisticated summarization for hybrid chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "RAG",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
